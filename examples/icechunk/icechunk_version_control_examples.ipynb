{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "25341568-feb1-46b7-acaf-d6b76f066e57",
   "metadata": {},
   "source": [
    "## Icechunk Version Control and Branching\n",
    "Showcase for adding new data over time to an icechunk store, \"time traveling\", and making new branches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f0fe850-0e5e-40e5-93b1-1c9f468a2d28",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "from icefabric.builds import IcechunkRepo, S3Path\n",
    "from icefabric.helpers import virtualize_and_concat_archival_files_on_time\n",
    "from icefabric.schemas import FileType, NGWPCLocations\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# Before running this cell, make sure you created a .env file in this directory with your AWS credentials in it\n",
    "# NOTE - if you authenticate with AWS SSO, leave it commented out\n",
    "load_dotenv()\n",
    "\n",
    "# Create icechunk repo at s3://hydrofabric-data/ic_testing/snodas_yearly_append_test\n",
    "# NOTE - make sure this S3 directory doesn't exist prior to running this cell\n",
    "new_repo_s3_path = S3Path(bucket=\"hydrofabric-data\", prefix=\"ic_testing/snodas_yearly_append_test\")\n",
    "new_repo = IcechunkRepo(location=new_repo_s3_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "61ff84b0-9dba-494e-b9c4-187923f504e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Snapshot ID:\tMWPZ0C0SCM3TRRKNF8D0\n",
      "Timestamp:\t2025-06-16 23:59:40.242436+00:00\n",
      "Message:\tRepository initialized\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Print repo ancestry\n",
    "new_repo.print_history()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef762477-5c6b-4876-b5a2-a708b9f3d02f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "zz_ssmv11034tS__T0001TTNATS2009*.nc\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Opening files as virtual datasets.: 100%|\u001b[38;2;55;182;189m███████████████████████████████████████████████████\u001b[0m| 5/5 [01:27<00:00, 17.52s/files]\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset is uploaded. Commit: 9CNDKXE65PVGCHK4CHBG\n"
     ]
    }
   ],
   "source": [
    "# Collect first five SNODAS netcdf files from 2009 and combine/virtualize them together into a single dataset\n",
    "snodas_09_vds = virtualize_and_concat_archival_files_on_time(\n",
    "    location=NGWPCLocations.SNODAS_REF.path,\n",
    "    file_date_pattern=\"zz_ssmv11034tS__T0001TTNATS*05HP001.nc\",\n",
    "    file_type=FileType.NETCDF,\n",
    "    manual_file_pattern=\"zz_ssmv11034tS__T0001TTNATS2009*.nc\",\n",
    "    loadable_vars=[\"crs\"],\n",
    "    testing_file_quantity=5,\n",
    ")\n",
    "\n",
    "# Add 09 data to SNODAS repo with a new snapshot\n",
    "new_repo.write_dataset(ds=snodas_09_vds, virtualized=True, commit=\"First commit! 09 data added.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b3b6e582-ba59-44c2-9753-74afa7c48fd0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Snapshot ID:\t9CNDKXE65PVGCHK4CHBG\n",
      "Timestamp:\t2025-06-17 00:01:17.067141+00:00\n",
      "Message:\tFirst commit! 09 data added.\n",
      "\n",
      "Snapshot ID:\tMWPZ0C0SCM3TRRKNF8D0\n",
      "Timestamp:\t2025-06-16 23:59:40.242436+00:00\n",
      "Message:\tRepository initialized\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Now that we have a new snapshot, reprint the repo ancestry\n",
    "new_repo.print_history()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "41d0467d-84da-45f2-94e2-f911e88bbf68",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<xarray.Dataset> Size: 671MB\n",
      "Dimensions:  (lon: 8192, time: 5, lat: 4096)\n",
      "Coordinates:\n",
      "  * lon      (lon) float64 66kB -130.5 -130.5 -130.5 ... -62.27 -62.26 -62.25\n",
      "  * lat      (lat) float64 33kB 24.1 24.11 24.12 24.13 ... 58.21 58.22 58.23\n",
      "  * time     (time) datetime64[ns] 40B 2009-12-09 2009-12-10 ... 2009-12-13\n",
      "Data variables:\n",
      "    Band1    (time, lat, lon) float32 671MB dask.array<chunksize=(1, 4096, 8192), meta=np.ndarray>\n",
      "    crs      (time) object 40B dask.array<chunksize=(5,), meta=np.ndarray>\n",
      "Attributes:\n",
      "    Conventions:  CF-1.5\n",
      "    GDAL:         GDAL 3.11.0dev-f1386937cde9e540784909294fdd45cda3ee65d2, re...\n",
      "    history:      Tue Feb 04 18:40:28 2025: GDAL CreateCopy( /data/unmasked/2...\n"
     ]
    }
   ],
   "source": [
    "# Print the data now contained within the SNODAS repo\n",
    "snodas_data = new_repo.retrieve_dataset()\n",
    "print(snodas_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbefb68a-b747-449e-b7da-4c61c6f4a2e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "zz_ssmv11034tS__T0001TTNATS2010*.nc\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Opening files as virtual datasets.: 100%|\u001b[38;2;55;182;189m███████████████████████████████████████████████████\u001b[0m| 5/5 [01:27<00:00, 17.54s/files]\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset has been appended on the time dimension. Commit: 6FMVKF4CS7N5RW6PC86G\n"
     ]
    }
   ],
   "source": [
    "# Much like the 09 SNODAS files were collected, do the same for 2010\n",
    "snodas_10_vds = virtualize_and_concat_archival_files_on_time(\n",
    "    location=NGWPCLocations.SNODAS_REF.path,\n",
    "    file_date_pattern=\"zz_ssmv11034tS__T0001TTNATS*05HP001.nc\",\n",
    "    file_type=FileType.NETCDF,\n",
    "    manual_file_pattern=\"zz_ssmv11034tS__T0001TTNATS2010*.nc\",\n",
    "    loadable_vars=[\"crs\"],\n",
    "    testing_file_quantity=5,\n",
    ")\n",
    "\n",
    "# Append 2010 data to SNODAS repo with a new snapshot\n",
    "new_repo.append_virt_data_to_store(\n",
    "    vds=snodas_10_vds, append_dim=\"time\", commit=\"Appended new data from the year 2010\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c5cfc9ee-0357-404f-8535-c414409ac58e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Snapshot ID:\t6FMVKF4CS7N5RW6PC86G\n",
      "Timestamp:\t2025-06-17 00:03:27.965202+00:00\n",
      "Message:\tAppended new data from the year 2010\n",
      "\n",
      "Snapshot ID:\t9CNDKXE65PVGCHK4CHBG\n",
      "Timestamp:\t2025-06-17 00:01:17.067141+00:00\n",
      "Message:\tFirst commit! 09 data added.\n",
      "\n",
      "Snapshot ID:\tMWPZ0C0SCM3TRRKNF8D0\n",
      "Timestamp:\t2025-06-16 23:59:40.242436+00:00\n",
      "Message:\tRepository initialized\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Now that we have another new snapshot with 2010 data, reprint the repo ancestry\n",
    "new_repo.print_history()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f9a3d55d-7d51-4317-9dc8-cf41316d3fea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<xarray.Dataset> Size: 1GB\n",
      "Dimensions:  (time: 10, lat: 4096, lon: 8192)\n",
      "Coordinates:\n",
      "  * lon      (lon) float64 66kB -130.5 -130.5 -130.5 ... -62.27 -62.26 -62.25\n",
      "  * time     (time) datetime64[ns] 80B 2009-12-09 2009-12-10 ... 2010-01-05\n",
      "  * lat      (lat) float64 33kB 24.1 24.11 24.12 24.13 ... 58.21 58.22 58.23\n",
      "Data variables:\n",
      "    Band1    (time, lat, lon) float32 1GB dask.array<chunksize=(1, 4096, 8192), meta=np.ndarray>\n",
      "    crs      (time) object 80B dask.array<chunksize=(5,), meta=np.ndarray>\n",
      "Attributes:\n",
      "    Conventions:  CF-1.5\n",
      "    GDAL:         GDAL 3.11.0dev-f1386937cde9e540784909294fdd45cda3ee65d2, re...\n",
      "    history:      Tue Feb 04 19:54:02 2025: GDAL CreateCopy( /data/unmasked/2...\n"
     ]
    }
   ],
   "source": [
    "# Print the new repo collection with both 2009 and 2010 data\n",
    "snodas_data = new_repo.retrieve_dataset()\n",
    "print(snodas_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e8c16d28-1933-48fd-8cfc-31dd18d8ed7d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<xarray.Dataset> Size: 671MB\n",
      "Dimensions:  (time: 5, lat: 4096, lon: 8192)\n",
      "Coordinates:\n",
      "  * lon      (lon) float64 66kB -130.5 -130.5 -130.5 ... -62.27 -62.26 -62.25\n",
      "  * lat      (lat) float64 33kB 24.1 24.11 24.12 24.13 ... 58.21 58.22 58.23\n",
      "  * time     (time) datetime64[ns] 40B 2009-12-09 2009-12-10 ... 2009-12-13\n",
      "Data variables:\n",
      "    Band1    (time, lat, lon) float32 671MB dask.array<chunksize=(1, 4096, 8192), meta=np.ndarray>\n",
      "    crs      (time) object 40B dask.array<chunksize=(5,), meta=np.ndarray>\n",
      "Attributes:\n",
      "    Conventions:  CF-1.5\n",
      "    GDAL:         GDAL 3.11.0dev-f1386937cde9e540784909294fdd45cda3ee65d2, re...\n",
      "    history:      Tue Feb 04 18:40:28 2025: GDAL CreateCopy( /data/unmasked/2...\n"
     ]
    }
   ],
   "source": [
    "# Retrieve and print the data from the previous snapshot, before 2010 data was added\n",
    "prev_snap_snodas_data = new_repo.retrieve_prev_snapshot()\n",
    "print(prev_snap_snodas_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d940692-0407-4c46-8df2-5a0036f4420b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "zz_ssmv11034tS__T0001TTNATS2011*.nc\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Opening files as virtual datasets.: 100%|\u001b[38;2;55;182;189m███████████████████████████████████████████████████\u001b[0m| 5/5 [01:28<00:00, 17.79s/files]\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset has been appended on the time dimension. Commit: T30V4R9GM9TCKHFFBQFG\n"
     ]
    }
   ],
   "source": [
    "# Make a new feature branch based on \"main\" to add 2011 data\n",
    "new_repo.create_new_branch(name=\"2011_feature\")\n",
    "\n",
    "# Much like the 09 SNODAS files were collected, do the same for 2010\n",
    "snodas_11_vds = virtualize_and_concat_archival_files_on_time(\n",
    "    location=NGWPCLocations.SNODAS_REF.path,\n",
    "    file_date_pattern=\"zz_ssmv11034tS__T0001TTNATS*05HP001.nc\",\n",
    "    file_type=FileType.NETCDF,\n",
    "    manual_file_pattern=\"zz_ssmv11034tS__T0001TTNATS2011*.nc\",\n",
    "    loadable_vars=[\"crs\"],\n",
    "    testing_file_quantity=5,\n",
    ")\n",
    "\n",
    "# Append 2011 data to SNODAS repo's new branch with a new snapshot\n",
    "new_repo.append_virt_data_to_store(\n",
    "    vds=snodas_11_vds, append_dim=\"time\", commit=\"Appended new data from the year 2011\", branch=\"2011_feature\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8386ab8e-02ef-4d87-8b55-91a26bb76675",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NEW BRANCH =====================================\n",
      "Snapshot ID:\tT30V4R9GM9TCKHFFBQFG\n",
      "Timestamp:\t2025-06-17 00:05:10.559344+00:00\n",
      "Message:\tAppended new data from the year 2011\n",
      "\n",
      "Snapshot ID:\t6FMVKF4CS7N5RW6PC86G\n",
      "Timestamp:\t2025-06-17 00:03:27.965202+00:00\n",
      "Message:\tAppended new data from the year 2010\n",
      "\n",
      "Snapshot ID:\t9CNDKXE65PVGCHK4CHBG\n",
      "Timestamp:\t2025-06-17 00:01:17.067141+00:00\n",
      "Message:\tFirst commit! 09 data added.\n",
      "\n",
      "Snapshot ID:\tMWPZ0C0SCM3TRRKNF8D0\n",
      "Timestamp:\t2025-06-16 23:59:40.242436+00:00\n",
      "Message:\tRepository initialized\n",
      "\n",
      "MAIN BRANCH ====================================\n",
      "Snapshot ID:\t6FMVKF4CS7N5RW6PC86G\n",
      "Timestamp:\t2025-06-17 00:03:27.965202+00:00\n",
      "Message:\tAppended new data from the year 2010\n",
      "\n",
      "Snapshot ID:\t9CNDKXE65PVGCHK4CHBG\n",
      "Timestamp:\t2025-06-17 00:01:17.067141+00:00\n",
      "Message:\tFirst commit! 09 data added.\n",
      "\n",
      "Snapshot ID:\tMWPZ0C0SCM3TRRKNF8D0\n",
      "Timestamp:\t2025-06-16 23:59:40.242436+00:00\n",
      "Message:\tRepository initialized\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Now that we have a new branch with new 2011 data, print the history of both branches\n",
    "print(\"NEW BRANCH =====================================\")\n",
    "new_repo.print_history(branch=\"2011_feature\")\n",
    "print(\"MAIN BRANCH ====================================\")\n",
    "new_repo.print_history(branch=\"main\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "43814441-7461-4d93-99f0-a839b9324d99",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NEW BRANCH ========================================================\n",
      "<xarray.Dataset> Size: 2GB\n",
      "Dimensions:  (time: 15, lon: 8192, lat: 4096)\n",
      "Coordinates:\n",
      "  * time     (time) datetime64[ns] 120B 2009-12-09 2009-12-10 ... 2011-01-05\n",
      "  * lon      (lon) float64 66kB -130.5 -130.5 -130.5 ... -62.27 -62.26 -62.25\n",
      "  * lat      (lat) float64 33kB 24.1 24.11 24.12 24.13 ... 58.21 58.22 58.23\n",
      "Data variables:\n",
      "    crs      (time) object 120B dask.array<chunksize=(5,), meta=np.ndarray>\n",
      "    Band1    (time, lat, lon) float32 2GB dask.array<chunksize=(1, 4096, 8192), meta=np.ndarray>\n",
      "Attributes:\n",
      "    Conventions:  CF-1.5\n",
      "    GDAL:         GDAL 3.11.0dev-f1386937cde9e540784909294fdd45cda3ee65d2, re...\n",
      "    history:      Tue Feb 04 19:00:12 2025: GDAL CreateCopy( /data/unmasked/2...\n",
      "===================================================================\n",
      "MAIN BRANCH =======================================================\n",
      "<xarray.Dataset> Size: 1GB\n",
      "Dimensions:  (time: 10, lat: 4096, lon: 8192)\n",
      "Coordinates:\n",
      "  * lon      (lon) float64 66kB -130.5 -130.5 -130.5 ... -62.27 -62.26 -62.25\n",
      "  * time     (time) datetime64[ns] 80B 2009-12-09 2009-12-10 ... 2010-01-05\n",
      "  * lat      (lat) float64 33kB 24.1 24.11 24.12 24.13 ... 58.21 58.22 58.23\n",
      "Data variables:\n",
      "    Band1    (time, lat, lon) float32 1GB dask.array<chunksize=(1, 4096, 8192), meta=np.ndarray>\n",
      "    crs      (time) object 80B dask.array<chunksize=(5,), meta=np.ndarray>\n",
      "Attributes:\n",
      "    Conventions:  CF-1.5\n",
      "    GDAL:         GDAL 3.11.0dev-f1386937cde9e540784909294fdd45cda3ee65d2, re...\n",
      "    history:      Tue Feb 04 19:54:02 2025: GDAL CreateCopy( /data/unmasked/2...\n",
      "===================================================================\n"
     ]
    }
   ],
   "source": [
    "# Print both branch's datasets - notice the new one has 2011 data\n",
    "snodas_data_feat_branch = new_repo.retrieve_dataset(branch=\"2011_feature\")\n",
    "print(\"NEW BRANCH ========================================================\")\n",
    "print(snodas_data_feat_branch)\n",
    "print(\"===================================================================\")\n",
    "print(\"MAIN BRANCH =======================================================\")\n",
    "print(snodas_data)\n",
    "print(\"===================================================================\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a69a8c8e-0003-4738-964e-7bd136412264",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Icechunk repo @ s3://hydrofabric-data/ic_testing/snodas_yearly_append_test in its entirety was successfully deleted.\n"
     ]
    }
   ],
   "source": [
    "# Cleanup - delete the test repo entirely\n",
    "new_repo.delete_repo(quiet=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da2aa6f1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
