{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Apache Iceberg Version Control for Hydrofabric and Streamflow Data\n",
    "\n",
    "### Overview\n",
    "\n",
    "This notebook demonstrates **enterprise-grade version control capabilities** for hydrological datasets using Apache Iceberg. We'll showcase how the hydrofabric and streamflow observations can be managed with full version control.\n",
    "\n",
    "#### What is Apache Iceberg?\n",
    "\n",
    "**Apache Iceberg** is a high-performance table format designed for large-scale data lakes. Unlike traditional file formats, Iceberg provides:\n",
    "\n",
    "- **Automatic snapshots** of every data change\n",
    "- **Time travel queries** to access historical versions\n",
    "- **ACID transactions** for data consistency\n",
    "- **Schema evolution** without breaking existing queries\n",
    "- **Query performance** through advanced indexing and pruning\n",
    "- **Complete audit trails** for regulatory compliance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyarrow as pa\n",
    "from pyiceberg.catalog import load_catalog\n",
    "\n",
    "from icefabric.helpers import load_creds, load_pyiceberg_config\n",
    "\n",
    "# dir is where the .env file is located\n",
    "load_creds()\n",
    "\n",
    "# Loading the local pyiceberg config settings\n",
    "pyiceberg_config = load_pyiceberg_config()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading SQL Catalog\n",
    "# This catalog can be downloaded by running the following commands with AWS creds:\n",
    "# python tools/iceberg/export_catalog.py --namespace conus_hf\n",
    "catalog = load_catalog(\n",
    "    name=\"sql\",\n",
    "    type=pyiceberg_config[\"catalog\"][\"sql\"][\"type\"],\n",
    "    uri=pyiceberg_config[\"catalog\"][\"sql\"][\"uri\"],\n",
    "    warehouse=pyiceberg_config[\"catalog\"][\"sql\"][\"warehouse\"],\n",
    ")\n",
    "\n",
    "# # Loading Glue Catalog\n",
    "# catalog = load_catalog(\"glue\", **{\n",
    "#     \"type\": \"glue\",\n",
    "#     \"glue.region\": \"us-east-1\"\n",
    "# })"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exploring the Data Catalog\n",
    "\n",
    "Apache Iceberg organizes data into **catalogs**, **namespaces**, and **tables** - similar to databases, schemas, and tables in traditional systems. However, each table maintains complete version history automatically.\n",
    "\n",
    "#### Hydrofabric Tables\n",
    "\n",
    "The `conus_hf` namespace contains hydrofabric layers associated with the CONUS-based geopackage\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "catalog.list_tables(\"conus_hf\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's examine the **hydrolocations** table and make some versioned additions. Below we'll see both the snapshots from the hydrolocations table, and actual geopackage layer exported to a pandas dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "table = catalog.load_table(\"conus_hf.hydrolocations\")\n",
    "table.inspect.snapshots()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = table.scan().to_pandas()\n",
    "df.tail()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Snapshot Analysis: Understanding Version History\n",
    "\n",
    "Each snapshot in Iceberg contains:\n",
    "- **Unique identifier** (snapshot_id)\n",
    "- **Summary metadata** describing the operation\n",
    "- **Timestamp** of the change\n",
    "- **File manifests** pointing to data files\n",
    "- **Schema information** at that point in time\n",
    "\n",
    "This enables **complete traceability** of how data evolved over time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for snapshot in table.snapshots():\n",
    "    print(f\"Snapshot ID: {snapshot.snapshot_id}; Summary:  {snapshot.summary}\")\n",
    "snapshot_id = table.metadata.snapshots[0].snapshot_id"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Demonstrating Version Control: Adding New Monitoring Location\n",
    "\n",
    "Now we'll demonstrate Iceberg's version control by adding a **new hydrologic monitoring location**\n",
    "\n",
    "#### The Version Control Process:\n",
    "\n",
    "1. **Modify data** (add new monitoring location)\n",
    "2. **Overwrite table** (creates new snapshot automatically)\n",
    "3. **Preserve history** (all previous versions remain accessible)\n",
    "4. **Track changes** (complete audit trail maintained)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_df = df.copy()\n",
    "new_df.loc[len(new_df)] = {\n",
    "    \"poi_id\": 99999,\n",
    "    \"id\": \"wb-0\",\n",
    "    \"nex_id\": \"tnx-0\",\n",
    "    \"hf_id\": 999999,\n",
    "    \"hl_link\": \"Testing\",\n",
    "    \"hl_reference\": \"testing\",\n",
    "    \"hl_uri\": \"testing\",\n",
    "    \"hl_source\": \"testing\",\n",
    "    \"hl_x\": -1.952088e06,\n",
    "    \"hl_y\": 1.283884e06,\n",
    "    \"vpu_id\": 18,\n",
    "}\n",
    "new_df.tail()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Writing Changes: Automatic Snapshot Creation\n",
    "\n",
    "When we write changes to an Iceberg table:\n",
    "\n",
    "1. **Schema validation** ensures data compatibility\n",
    "2. **New snapshot created** automatically with unique ID\n",
    "3. **Previous snapshots preserved** for time travel\n",
    "4. **Metadata updated** with operation summary\n",
    "5. **ACID guarantees** ensure consistency\n",
    "\n",
    "This happens **atomically** - either the entire operation succeeds or fails, with no partial states.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_df = pa.Table.from_pandas(new_df, preserve_index=False)\n",
    "with table.update_schema() as update_schema:\n",
    "    update_schema.union_by_name(_df.schema)\n",
    "table.overwrite(_df)\n",
    "table.scan().to_pandas().tail()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Verifying New Snapshot Creation\n",
    "\n",
    "Let's examine the updated snapshot history. Notice how we now have **multiple snapshots**:\n",
    "\n",
    "1. **Original data** (initial snapshot)\n",
    "2. **Data with new location** (our recent addition)\n",
    "\n",
    "Each snapshot is **completely independent** and can be accessed separately for different analyses or rollback scenarios.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for snapshot in table.snapshots():\n",
    "    print(f\"Snapshot ID: {snapshot.snapshot_id}; Summary:  {snapshot.summary}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Iceberg's **time travel capability** allows querying any previous snapshot using its ID\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "snapshot_id = table.metadata.snapshots[0].snapshot_id\n",
    "snapshot_id_latest = table.metadata.snapshots[-1].snapshot_id\n",
    "table.scan(snapshot_id=snapshot_id).to_pandas().tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "table.scan(snapshot_id=snapshot_id_latest).to_pandas().tail()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Comparing Versions: Before and After\n",
    "\n",
    "Notice the difference between snapshots:\n",
    "- **Original snapshot**: Contains original monitoring locations\n",
    "- **Latest snapshot**: Includes our new test location (poi_id: 99999)\n",
    "\n",
    "This demonstrates **non-destructive updates** - both versions coexist and remain queryable.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Streamflow Observations: Time Series Version Control\n",
    "\n",
    "Now let's examine **streamflow observations** - time series data that requires different version control considerations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "table = catalog.load_table(\"streamflow_observations.usgs_hourly\")\n",
    "table.inspect.snapshots()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = table.scan().to_pandas().set_index(\"time\")\n",
    "df.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for snapshot in table.snapshots():\n",
    "    print(f\"Snapshot ID: {snapshot.snapshot_id}; Summary:  {snapshot.summary}\")\n",
    "snapshot_id = table.metadata.snapshots[0].snapshot_id"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Adding Time Series Data: Simulating Real-Time Updates\n",
    "\n",
    "We'll now add a new streamflow observation to demonstrate version control for time series data\n",
    "\n",
    "The process maintains **historical context** while adding new information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_streamflow_df = df.copy()\n",
    "new_streamflow_df.loc[len(new_df)] = 0.1\n",
    "new_streamflow_df.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_df = pa.Table.from_pandas(new_streamflow_df)\n",
    "with table.update_schema() as update_schema:\n",
    "    update_schema.union_by_name(_df.schema)\n",
    "table.overwrite(_df)\n",
    "table.scan().to_pandas().tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for snapshot in table.snapshots():\n",
    "    print(f\"Snapshot ID: {snapshot.snapshot_id}; Summary:  {snapshot.summary}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Time Travel with Time Series Data\n",
    "\n",
    "Comparing different snapshots of time series data reveals:\n",
    "\n",
    "#### Original Snapshot (Baseline Data):\n",
    "- Contains original observational record\n",
    "- Represents specific quality control state\n",
    "- Suitable for historical analysis\n",
    "\n",
    "#### Latest Snapshot (Updated Data):  \n",
    "- Includes new observations\n",
    "- Represents current operational state\n",
    "- Suitable for real-time applications"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "snapshot_id = table.metadata.snapshots[0].snapshot_id\n",
    "snapshot_id_latest = table.metadata.snapshots[-1].snapshot_id\n",
    "table.scan(snapshot_id=snapshot_id).to_pandas().tail().set_index(\"time\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = table.scan(snapshot_id=snapshot_id).to_pandas()\n",
    "_df = pa.Table.from_pandas(df)\n",
    "with table.update_schema() as update_schema:\n",
    "    update_schema.union_by_name(_df.schema)\n",
    "table.overwrite(_df)\n",
    "table.scan().to_pandas().tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "table.scan(snapshot_id=snapshot_id_latest).to_pandas().tail().set_index(\"time\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Demonstration Cleanup: Reverting Changes\n",
    "\n",
    "To maintain data integrity, we'll now **revert our test changes** by removing the added records. This demonstrates:\n",
    "\n",
    "- **Controlled rollback** procedures\n",
    "- **Data management** best practices  \n",
    "- **Cleanup workflows** for testing environments\n",
    "\n",
    "**Important**: Even these cleanup operations create new snapshots, maintaining complete audit trails of all activities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cleaning up hydrofabric changes\n",
    "table = catalog.load_table(\"conus_hf.hydrolocations\")\n",
    "new_df = new_df.drop(new_df.index[-1])\n",
    "_df = pa.Table.from_pandas(new_df, preserve_index=False)\n",
    "with table.update_schema() as update_schema:\n",
    "    update_schema.union_by_name(_df.schema)\n",
    "table.overwrite(_df)\n",
    "catalog.load_table(\"conus_hf.hydrolocations\").scan().to_pandas().tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cleaning up Streamflow Observation changes\n",
    "table = catalog.load_table(\"streamflow_observations.usgs_hourly\")\n",
    "new_streamflow_df = new_streamflow_df.drop(new_streamflow_df.index[-1])\n",
    "_df = pa.Table.from_pandas(new_streamflow_df)\n",
    "with table.update_schema() as update_schema:\n",
    "    update_schema.union_by_name(_df.schema)\n",
    "table.overwrite(_df)\n",
    "catalog.load_table(\"streamflow_observations.usgs_hourly\").scan().to_pandas().tail()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**This demonstration showcases Apache Iceberg's capability to provide version control for water resources data, enabling both reliability and reproducibility for large-scale hydrological modeling systems.**"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
