{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0",
   "metadata": {},
   "source": [
    "# Demo: Timetravel in with iceberg tables\n",
    "\n",
    "Create a demo catalog, make changes, and see the changes with \"snapshot\" history.\n",
    "\n",
    "Requires:\n",
    "- pyiceberg[sql-sqlite] installed\n",
    "- `.env` with your AWS credentials"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "from pyiceberg.catalog import load_catalog\n",
    "\n",
    "from icefabric.helpers import load_creds, load_pyiceberg_config\n",
    "\n",
    "# Changes the current working dir to be the project root\n",
    "current_working_dir = Path.cwd()\n",
    "os.chdir(Path.cwd() / \"../../\")\n",
    "print(\n",
    "    f\"Changed current working dir from {current_working_dir} to: {Path.cwd()}. This must run at the project root\"\n",
    ")\n",
    "\n",
    "\n",
    "# dir is where the .env file is located\n",
    "load_creds(dir=Path.cwd())\n",
    "\n",
    "# Loading the local pyiceberg config settings\n",
    "pyiceberg_config = load_pyiceberg_config(Path.cwd())\n",
    "catalog = load_catalog(\n",
    "    name=\"sql\",\n",
    "    type=pyiceberg_config[\"catalog\"][\"sql\"][\"type\"],\n",
    "    uri=pyiceberg_config[\"catalog\"][\"sql\"][\"uri\"],\n",
    "    warehouse=pyiceberg_config[\"catalog\"][\"sql\"][\"warehouse\"],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2",
   "metadata": {},
   "source": [
    "Load a specific table that we would like to time travel from"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3",
   "metadata": {},
   "outputs": [],
   "source": [
    "table = catalog.load_table(\"streamflow_observations.usgs_hourly\")\n",
    "table.inspect.snapshots()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4",
   "metadata": {},
   "source": [
    "Let's view this data and see what's there"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = table.scan().to_pandas().set_index(\"time\")\n",
    "df.tail()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6",
   "metadata": {},
   "source": [
    "A snapshot was created for the initial append.  Store this snapshot id for later use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7",
   "metadata": {},
   "outputs": [],
   "source": [
    "for snapshot in table.snapshots():\n",
    "    print(f\"Snapshot ID: {snapshot.snapshot_id}; Summary:  {snapshot.summary}\")\n",
    "snapshot_id = table.metadata.snapshots[0].snapshot_id"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8",
   "metadata": {},
   "source": [
    "Add a new column for a fake gauge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "n = len(df)\n",
    "x = np.linspace(0, n, n)\n",
    "y = np.sin(2 * np.pi * 1 * x / n).astype(np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyarrow as pa\n",
    "\n",
    "df[\"12345678\"] = y\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11",
   "metadata": {},
   "source": [
    "There should be a new \"12345678\" column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12",
   "metadata": {},
   "outputs": [],
   "source": [
    "table.schema"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13",
   "metadata": {},
   "source": [
    "There should now be three snapshots.  The original, a delete, and an append with the new column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14",
   "metadata": {},
   "outputs": [],
   "source": [
    "for snapshot in table.snapshots():\n",
    "    print(f\"Snapshot ID: {snapshot.snapshot_id}; Summary:  {snapshot.summary}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15",
   "metadata": {},
   "source": [
    "You can use the scan function and the first snapshot ID (this variable was saved earlier) to look at the table before the\n",
    "new column was added.  This table doesn't have 12345678."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16",
   "metadata": {},
   "outputs": [],
   "source": [
    "table.scan(snapshot_id=snapshot_id).to_pandas().tail()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17",
   "metadata": {},
   "source": [
    "whereas loading without the snapshot gives the latest data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18",
   "metadata": {},
   "outputs": [],
   "source": [
    "table.scan().to_pandas().tail()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19",
   "metadata": {},
   "source": [
    "Now, let's delete that data from the local warehouse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.drop(\"12345678\", axis=1)\n",
    "df.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21",
   "metadata": {},
   "outputs": [],
   "source": [
    "with table.update_schema() as update_schema:\n",
    "    update_schema.delete_column(\"12345678\")\n",
    "\n",
    "# Then overwrite with the data (without the column)\n",
    "df = df.drop(\"12345678\", axis=1)\n",
    "_df = pa.Table.from_pandas(df)\n",
    "table.overwrite(_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22",
   "metadata": {},
   "outputs": [],
   "source": [
    "_df.schema"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23",
   "metadata": {},
   "source": [
    "Let's now check the snapshots and pull in the latest data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24",
   "metadata": {},
   "outputs": [],
   "source": [
    "for snapshot in table.snapshots():\n",
    "    print(f\"Snapshot ID: {snapshot.snapshot_id}; Summary:  {snapshot.summary}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25",
   "metadata": {},
   "outputs": [],
   "source": [
    "table.scan().to_pandas().tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26",
   "metadata": {},
   "outputs": [],
   "source": [
    "snapshot_id = table.snapshots()[-3].snapshot_id\n",
    "table.scan(snapshot_id=snapshot_id).to_pandas().tail()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "icefabric",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
