{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c015f447",
   "metadata": {},
   "source": [
    "# Demo: Timetravel in with iceberg tables\n",
    "\n",
    "Create a demo catalog, make changes, and see the changes with \"snapshot\" history.\n",
    "\n",
    "Requires:\n",
    "- pyiceberg[sql-sqlite] installed\n",
    "- `.env` with your AWS credentials"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9f07637f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import shlex\n",
    "import subprocess\n",
    "from pathlib import Path\n",
    "\n",
    "import pyarrow.parquet as pq\n",
    "from pyiceberg.catalog import load_catalog"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "db86d0ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "from icefabric.helpers import load_creds\n",
    "\n",
    "# dir is where the .env file is located\n",
    "load_creds(dir=Path.cwd().parents[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3f901988",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "download: s3://ngwpc-hydrofabric/hydrofabric_parquet/2.2/CONUS/divides.parquet to data/parquet/divides.parquet\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "CompletedProcess(args=['aws', 's3', 'cp', 's3://ngwpc-hydrofabric/hydrofabric_parquet/2.2/CONUS/divides.parquet', 'data/parquet'], returncode=0)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Retrieve sample parquet from s3\n",
    "parquet_path = Path(\"./data/parquet\")\n",
    "parquet_path.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "subprocess.run(\n",
    "    shlex.split(\n",
    "        f\"aws s3 cp s3://ngwpc-hydrofabric/hydrofabric_parquet/2.2/CONUS/divides.parquet {parquet_path}\"\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f41913d7-1cc8-42d0-8cea-053b4c23d6bb",
   "metadata": {},
   "source": [
    "Read divides parquet file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6651c9e7-c38d-46d1-b6a4-491baf12cc90",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pq.read_table(\"data/parquet/divides.parquet\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e4d5aaf-d51c-486c-86ac-32eeb2b7e74f",
   "metadata": {},
   "source": [
    "Create data catalog stored in \"warehouse\" directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "96974839-0308-4256-bb3e-0cddfea9d037",
   "metadata": {},
   "outputs": [],
   "source": [
    "warehouse_path = Path(\"./warehouse\")\n",
    "warehouse_path.mkdir(exist_ok=True)\n",
    "catalog = load_catalog(\n",
    "    \"default\",\n",
    "    **{\n",
    "        \"type\": \"sql\",\n",
    "        \"uri\": f\"sqlite:///{warehouse_path}/pyiceberg_catalog.db\",\n",
    "        \"warehouse\": f\"file://{warehouse_path}\",\n",
    "    },\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fe13a4d-0c18-4f2b-953e-a9c658b7225a",
   "metadata": {},
   "source": [
    "Create Iceberg table for divides"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "efed8866-134c-40f9-969b-1e4c20b5f11b",
   "metadata": {},
   "outputs": [],
   "source": [
    "catalog.create_namespace(\"default\")\n",
    "table = catalog.create_table(\n",
    "    \"default.divides\",\n",
    "    schema=df.schema,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f29ed9b6-ba8d-4a54-bbc5-eec900f1cec6",
   "metadata": {},
   "source": [
    "Add divides data to Iceberg table and print the number of rows.  There should be 831777 divides for CONUS."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0a9d4717-9055-4ac7-819c-aca6c2435c95",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "831777"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "table.append(df)\n",
    "len(table.scan().to_arrow())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1c2b449-667a-4bcd-9c94-d0e604c33f8c",
   "metadata": {},
   "source": [
    "A snapshot was created for the initial append.  Store this snapshot id for later use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "78ee6462-5db1-4a40-89b8-3837e63711cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Snapshot ID: 6392225911450853109; Summary:  operation=Operation.APPEND\n"
     ]
    }
   ],
   "source": [
    "for snapshot in table.snapshots():\n",
    "    print(f\"Snapshot ID: {snapshot.snapshot_id}; Summary:  {snapshot.summary}\")\n",
    "snapshot_id = table.metadata.snapshots[0].snapshot_id"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f245529-a3c9-4a99-bcb3-fe1078779498",
   "metadata": {},
   "source": [
    "Add a new column for flowpath length in m.  Overwrite original table."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "975224d2-deb0-420f-be80-6b5cac7bbe60",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyarrow.compute as pc\n",
    "\n",
    "df = df.append_column(\"lengthm\", pc.multiply(df[\"lengthkm\"], 1000))\n",
    "with table.update_schema() as update_schema:\n",
    "    update_schema.union_by_name(df.schema)\n",
    "table.overwrite(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3ff7a72-70f6-44e8-a6da-d1c9766c6782",
   "metadata": {},
   "source": [
    "There should be a new \"lengthm\" column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d826f290-79bf-4bab-a508-cd73bb3aadb6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<bound method Table.schema of divides(\n",
       "  1: divide_id: optional string,\n",
       "  2: toid: optional string,\n",
       "  3: type: optional string,\n",
       "  4: ds_id: optional double,\n",
       "  5: areasqkm: optional double,\n",
       "  6: vpuid: optional string,\n",
       "  7: id: optional string,\n",
       "  8: lengthkm: optional double,\n",
       "  9: tot_drainage_areasqkm: optional double,\n",
       "  10: has_flowline: optional boolean,\n",
       "  11: geometry: optional binary,\n",
       "  12: lengthm: optional double\n",
       "),\n",
       "partition by: [],\n",
       "sort order: [],\n",
       "snapshot: Operation.APPEND: id=6082380623201209864, parent_id=3770483438741118773, schema_id=1>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "table.schema"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "668d3f57-e9ef-47e7-a268-05699debcf8f",
   "metadata": {},
   "source": [
    "There should now be three snapshots.  The original, a delete, and an append with the new column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "6b3a70f2-2271-4bcd-b5fb-83683f36705c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Snapshot ID: 6392225911450853109; Summary:  operation=Operation.APPEND\n",
      "Snapshot ID: 3770483438741118773; Summary:  operation=Operation.DELETE\n",
      "Snapshot ID: 6082380623201209864; Summary:  operation=Operation.APPEND\n"
     ]
    }
   ],
   "source": [
    "for snapshot in table.snapshots():\n",
    "    print(f\"Snapshot ID: {snapshot.snapshot_id}; Summary:  {snapshot.summary}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3685eddd-31bb-4c07-a656-6cfb5e97881a",
   "metadata": {},
   "source": [
    "You can use the scan function and the first snapshot ID (this variable was saved earlier) to look at the table before the\n",
    "new column was added.  This table doesn't have lengthm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "2f47f9e3-7e6b-468b-a978-2c433e662c12",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pyarrow.Table\n",
      "divide_id: large_string\n",
      "toid: large_string\n",
      "type: large_string\n",
      "ds_id: double\n",
      "areasqkm: double\n",
      "vpuid: large_string\n",
      "id: large_string\n",
      "lengthkm: double\n",
      "tot_drainage_areasqkm: double\n",
      "has_flowline: bool\n",
      "geometry: large_binary\n"
     ]
    }
   ],
   "source": [
    "# scan = table.scan(row_filter=\"divide_id\" == \"cat-276\", selected_fields=('divide_id', 'lengthm')).to_arrow()\n",
    "# print(scan)\n",
    "print(table.scan(snapshot_id=snapshot_id).to_arrow().to_string())\n",
    "# table.scan(snapshot_id=snapshot_id)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "icefabric",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
