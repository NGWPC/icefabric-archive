{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0",
   "metadata": {},
   "source": [
    "# Demo: Time Travel with Iceberg Tables - CRUD Operations & Version Control\n",
    "\n",
    "## Overview\n",
    "This notebook demonstrates **Create, Read, Update, and Delete (CRUD) operations** on version-controlled data using Apache Iceberg tables. The notebook showcases how Iceberg's snapshot-based architecture enables time travel capabilities and maintains a complete history of all data modifications.\n",
    "\n",
    "## Key Features Demonstrated:\n",
    "- **CREATE**: Creating new tables and adding data\n",
    "- **READ**: Querying current and historical data snapshots\n",
    "- **UPDATE**: Modifying table schemas and data\n",
    "- **DELETE**: Removing columns and dropping tables\n",
    "- **VERSION CONTROL**: Time travel through snapshots to view historical states\n",
    "\n",
    "## Prerequisites:\n",
    "- a local pyiceberg catalog spun up and referenced through .pyiceberg.yaml\n",
    "\n",
    "## Objectives:\n",
    "By the end of this notebook, you will understand how to:\n",
    "1. Perform all CRUD operations on Iceberg tables\n",
    "2. Leverage version control to access historical data states\n",
    "3. Create and manage table snapshots\n",
    "4. Navigate between different versions of your data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "from pyiceberg.catalog import load_catalog\n",
    "\n",
    "from icefabric.helpers import load_creds, load_pyiceberg_config\n",
    "\n",
    "# Changes the current working dir to be the project root\n",
    "current_working_dir = Path.cwd()\n",
    "os.chdir(Path.cwd() / \"../../\")\n",
    "print(\n",
    "    f\"Changed current working dir from {current_working_dir} to: {Path.cwd()}. This must run at the project root\"\n",
    ")\n",
    "\n",
    "\n",
    "# dir is where the .env file is located\n",
    "load_creds(dir=Path.cwd())\n",
    "\n",
    "# Loading the local pyiceberg config settings\n",
    "pyiceberg_config = load_pyiceberg_config(Path.cwd())\n",
    "catalog = load_catalog(\n",
    "    name=\"sql\",\n",
    "    type=pyiceberg_config[\"catalog\"][\"sql\"][\"type\"],\n",
    "    uri=pyiceberg_config[\"catalog\"][\"sql\"][\"uri\"],\n",
    "    warehouse=pyiceberg_config[\"catalog\"][\"sql\"][\"warehouse\"],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2",
   "metadata": {},
   "source": [
    "### READ Operation: Loading and Inspecting Existing Data\n",
    "\n",
    "We begin by demonstrating the **READ** operation by loading an existing table and examining its version history. This shows how Iceberg maintains complete metadata about all snapshots (versions) of the data.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3",
   "metadata": {},
   "outputs": [],
   "source": [
    "table = catalog.load_table(\"streamflow_observations.usgs_hourly\")\n",
    "table.inspect.snapshots()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4",
   "metadata": {},
   "source": [
    "Let's examine the current data in the table. This represents the latest version of our dataset. Notice how we can easily convert Iceberg tables to pandas DataFrames for analysis.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = table.scan().to_pandas().set_index(\"time\")\n",
    "df.tail()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6",
   "metadata": {},
   "source": [
    "### Version Control: Capturing Initial State\n",
    "\n",
    "**Version Control Feature**: Every operation in Iceberg creates a snapshot with a unique ID. We're capturing the initial snapshot ID here so we can demonstrate time travel capabilities later. This snapshot represents the baseline state of our data before any modifications.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7",
   "metadata": {},
   "outputs": [],
   "source": [
    "for snapshot in table.snapshots():\n",
    "    print(f\"Snapshot ID: {snapshot.snapshot_id}; Summary:  {snapshot.summary}\")\n",
    "snapshot_id = table.metadata.snapshots[0].snapshot_id"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8",
   "metadata": {},
   "source": [
    "### UPDATE Operation: Schema Evolution and Data Modification\n",
    " \n",
    "Now we'll demonstrate the **UPDATE** operation by adding a new column to our existing table. This involves:\n",
    "1. Creating synthetic data for the new column\n",
    "2. Updating the table schema to accommodate the new column\n",
    "3. Overwriting the table with the updated data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "n = len(df)\n",
    "x = np.linspace(0, n, n)\n",
    "y = np.sin(2 * np.pi * 1 * x / n).astype(np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyarrow as pa\n",
    "\n",
    "df[\"12345678\"] = y\n",
    "df.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11",
   "metadata": {},
   "outputs": [],
   "source": [
    "_df = pa.Table.from_pandas(df)\n",
    "with table.update_schema() as update_schema:\n",
    "    update_schema.union_by_name(_df.schema)\n",
    "table.overwrite(_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12",
   "metadata": {},
   "source": [
    "After our UPDATE operation, we can verify that the schema has been modified. The new column \"12345678\" should now be part of the table structure.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13",
   "metadata": {},
   "outputs": [],
   "source": [
    "table.schema().fields[-1]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14",
   "metadata": {},
   "source": [
    "### Version Control: Tracking All Changes\n",
    "\n",
    "**Version Control Feature**: Notice how Iceberg has automatically created new snapshots for our UPDATE operation. The snapshot history now shows:\n",
    "- Original data snapshot\n",
    "- Delete operation snapshot (part of overwrite)\n",
    "- New append operation snapshot (with the new column)\n",
    "\n",
    "This complete audit trail is essential for data governance and debugging."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15",
   "metadata": {},
   "outputs": [],
   "source": [
    "for snapshot in table.snapshots():\n",
    "    print(f\"Snapshot ID: {snapshot.snapshot_id}; Summary:  {snapshot.summary}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16",
   "metadata": {},
   "source": [
    "**Time Travel Feature**: Using the snapshot ID we captured earlier, we can query the table as it existed before our UPDATE operation. This demonstrates Iceberg's powerful time travel capabilities - you can access any historical state of your data.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17",
   "metadata": {},
   "outputs": [],
   "source": [
    "table.scan(snapshot_id=snapshot_id).to_pandas().tail()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18",
   "metadata": {},
   "source": [
    "Comparing the current state (with the new column) versus the historical state (without the column) demonstrates how version control preserves all data states while allowing easy access to current data.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19",
   "metadata": {},
   "outputs": [],
   "source": [
    "table.scan().to_pandas().tail()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20",
   "metadata": {},
   "source": [
    "Now we'll demonstrate another **UPDATE** operation by removing the column we just added. This shows how Iceberg handles schema evolution in both directions (adding and removing columns).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21",
   "metadata": {},
   "outputs": [],
   "source": [
    "with table.update_schema() as update_schema:\n",
    "    update_schema.delete_column(\"12345678\")\n",
    "\n",
    "df = df.drop(\"12345678\", axis=1)\n",
    "_df = pa.Table.from_pandas(df)\n",
    "table.overwrite(_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22",
   "metadata": {},
   "outputs": [],
   "source": [
    "table.schema().fields[-1]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23",
   "metadata": {},
   "source": [
    "### CREATE Operation: Building New Tables\n",
    "\n",
    "Now we'll demonstrate the **CREATE** operation by building an entirely new table from scratch. This shows how to:\n",
    "1. Prepare data for a new table\n",
    "2. Create the table structure in the catalog\n",
    "3. Populate the table with initial data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24",
   "metadata": {},
   "outputs": [],
   "source": [
    "__df = df.copy()\n",
    "__df[\"12345678\"] = y\n",
    "subset_df = __df[[\"12345678\"]].copy()\n",
    "subset_df.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25",
   "metadata": {},
   "outputs": [],
   "source": [
    "namespace = \"streamflow_observations\"\n",
    "table_name = \"testing_hourly\"\n",
    "arrow_table = pa.Table.from_pandas(subset_df)\n",
    "iceberg_table = catalog.create_table(\n",
    "    f\"{namespace}.{table_name}\",\n",
    "    schema=arrow_table.schema,\n",
    ")\n",
    "iceberg_table.append(arrow_table)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26",
   "metadata": {},
   "source": [
    "### READ Operation: Verifying New Table Creation \n",
    "\n",
    "After our **CREATE** operation, we can verify that the new table exists in our namespace and examine its initial snapshot. Every new table starts with its first snapshot upon creation.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27",
   "metadata": {},
   "outputs": [],
   "source": [
    "catalog.list_tables(namespace)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28",
   "metadata": {},
   "outputs": [],
   "source": [
    "table = catalog.load_table(f\"{namespace}.{table_name}\")\n",
    "table.inspect.snapshots()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29",
   "metadata": {},
   "outputs": [],
   "source": [
    "table.scan().to_pandas().tail()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30",
   "metadata": {},
   "source": [
    "### DELETE Operation: Table Removal\n",
    "\n",
    "Finally, we demonstrate the **DELETE** operation by completely removing the table we just created. This shows how to clean up resources and manage table lifecycle.\n",
    "\n",
    "**Important**: Unlike column deletion (which is reversible through time travel), table deletion is permanent and removes all snapshots and data.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31",
   "metadata": {},
   "outputs": [],
   "source": [
    "catalog.drop_table(f\"{namespace}.{table_name}\")\n",
    "catalog.list_tables(namespace)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32",
   "metadata": {},
   "source": [
    "### Summary: CRUD Operations and Version Control Demonstrated\n",
    " \n",
    "This notebook has successfully demonstrated all required CRUD operations with version-controlled data:\n",
    " \n",
    "#### CREATE Operations:\n",
    "- Created new tables with `catalog.create_table()`\n",
    "- Added new columns to existing tables\n",
    "- Populated tables with initial data using `append()`\n",
    "\n",
    "#### READ Operations:\n",
    "- Loaded existing tables with `catalog.load_table()`\n",
    "- Queried current data states with `table.scan()`\n",
    "- Accessed historical data states using snapshot IDs\n",
    "- Inspected table schemas and metadata\n",
    " \n",
    "#### UPDATE Operations:\n",
    "- Modified table schemas by adding columns\n",
    "- Updated data through `overwrite()` operations\n",
    "- Removed columns from existing tables\n",
    "\n",
    "#### DELETE Operations:\n",
    "- Deleted columns from table schemas\n",
    "- Removed entire tables with `catalog.drop_table()`\n",
    "\n",
    "#### Version Control Features:\n",
    "- **Snapshot Management**: Every operation creates tracked snapshots\n",
    "- **Time Travel**: Access any historical state using snapshot IDs\n",
    "- **Audit Trail**: Complete history of all table modifications\n",
    "- **Schema Evolution**: Track changes to table structure over time\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "icefabric",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
