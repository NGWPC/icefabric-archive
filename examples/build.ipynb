{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyiceberg.catalog import load_catalog\n",
    "import pyarrow.parquet as pq\n",
    "import os\n",
    "from pathlib import Path\n",
    "import geopandas as gpd\n",
    "from shapely import wkb\n",
    "import numpy as np\n",
    "import networkx as nx\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyiceberg.catalog import load_catalog\n",
    "\n",
    "warehouse_path = \"../data/warehouse\"\n",
    "catalog = load_catalog(\n",
    "    \"default\",\n",
    "    **{\n",
    "        'type': 'sql',\n",
    "        \"uri\": f\"sqlite:///{warehouse_path}/icefabric_catalog.db\",\n",
    "        \"warehouse\": f\"file://{warehouse_path}\",\n",
    "    },\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_parquet_to_catalog(file_path, table_name):\n",
    "    # Check if table already exists\n",
    "    if catalog.table_exists(f\"hydrofabric.{table_name}\"):\n",
    "        print(f\"Table {table_name} already exists, loading it\")\n",
    "        return catalog.load_table(f\"hydrofabric.{table_name}\")\n",
    "    \n",
    "    # Read the parquet file\n",
    "    arrow_table = pq.read_table(file_path)\n",
    "    \n",
    "    # Create the table in the catalog\n",
    "    iceberg_table = catalog.create_table(\n",
    "        f\"hydrofabric.{table_name}\",\n",
    "        schema=arrow_table.schema,\n",
    "    )\n",
    "    \n",
    "    # Append the data to the table\n",
    "    iceberg_table.append(arrow_table)\n",
    "    \n",
    "    print(f\"Added {file_path} as table {table_name}\")\n",
    "    return iceberg_table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Table network already exists, loading it\n",
      "Table nexus already exists, loading it\n",
      "Table flowpath-attributes already exists, loading it\n",
      "Table divides already exists, loading it\n",
      "Table pois already exists, loading it\n",
      "Table flowpath-attributes-ml already exists, loading it\n",
      "Table divide-attributes already exists, loading it\n",
      "Table flowpaths already exists, loading it\n",
      "Table hydrolocations already exists, loading it\n",
      "Table lakes already exists, loading it\n"
     ]
    }
   ],
   "source": [
    "# Get all parquet files from the directory\n",
    "parquet_dir = \"../data/parquet\"\n",
    "parquet_files = list(Path(parquet_dir).glob(\"*.parquet\"))\n",
    "\n",
    "# Dictionary to store all tables\n",
    "tables = {}\n",
    "\n",
    "# Add each parquet file to the catalog\n",
    "for parquet_file in parquet_files:\n",
    "    table_name = parquet_file.stem  # Get filename without extension\n",
    "    tables[table_name] = add_parquet_to_catalog(str(parquet_file), table_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tables in the catalog:\n",
      "- ('hydrofabric', 'divide-attributes')\n",
      "- ('hydrofabric', 'divides')\n",
      "- ('hydrofabric', 'flowpath-attributes')\n",
      "- ('hydrofabric', 'flowpath-attributes-ml')\n",
      "- ('hydrofabric', 'flowpaths')\n",
      "- ('hydrofabric', 'hydrolocations')\n",
      "- ('hydrofabric', 'lakes')\n",
      "- ('hydrofabric', 'network')\n",
      "- ('hydrofabric', 'nexus')\n",
      "- ('hydrofabric', 'pois')\n"
     ]
    }
   ],
   "source": [
    "print(\"Tables in the catalog:\")\n",
    "for table_id in catalog.list_tables(\"hydrofabric\"):\n",
    "    print(f\"- {table_id}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_geodataframe(df):\n",
    "    if 'geometry' in df.columns:\n",
    "        df['geometry'] = df['geometry'].apply(\n",
    "            lambda x: wkb.loads(x) if x is not None else None\n",
    "        )\n",
    "        return gpd.GeoDataFrame(df, geometry='geometry', crs=\"EPSG:5070\")\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_origin(network_table, identifier, id_type=\"hl_uri\"):\n",
    "    \"\"\"Find an origin point in the hydrofabric network.\n",
    "    \n",
    "    This function handles the case where multiple records match the identifier.\n",
    "    It follows the R implementation to select a single origin point based on \n",
    "    the minimum hydroseq value.\n",
    "    \"\"\"\n",
    "    # Filter network table by the identifier\n",
    "    if id_type == \"hl_uri\":\n",
    "        row_filter = f\"{id_type} == '{identifier}'\"\n",
    "    # elif id_type == \"comid\":\n",
    "    #     row_filter = f\"hf_id == {identifier}\"\n",
    "    # elif id_type == \"id\":\n",
    "    #     row_filter = f\"id == '{identifier}'\"\n",
    "    # elif id_type == \"poi_id\":\n",
    "    #     row_filter = f\"poi_id == '{identifier}'\"\n",
    "    else:\n",
    "        raise ValueError(f\"Identifier type {id_type} not supported\")\n",
    "    \n",
    "    # Get all matching records\n",
    "    origin_candidates = network_table.scan(row_filter=row_filter).to_pandas()\n",
    "    \n",
    "    if len(origin_candidates) == 0:\n",
    "        raise ValueError(f\"No origin found for {id_type}='{identifier}'\")\n",
    "    \n",
    "    # Select relevant columns for the origin\n",
    "    origin_cols = ['id', 'toid', 'vpuid', 'topo', 'hydroseq']\n",
    "    available_cols = [col for col in origin_cols if col in origin_candidates.columns]\n",
    "    \n",
    "    # Select only the relevant columns and drop duplicates\n",
    "    origin = origin_candidates[available_cols].drop_duplicates()\n",
    "    \n",
    "    # Find the record with minimum hydroseq\n",
    "    if 'hydroseq' in origin.columns:\n",
    "        # For consistency with R, check if there are unique hydroseq values\n",
    "        if len(origin['hydroseq'].unique()) > 1:\n",
    "            # Sort by hydroseq and take the minimum\n",
    "            origin = origin.sort_values('hydroseq').iloc[0:1]\n",
    "        \n",
    "    # If we still have multiple records, it's a problem\n",
    "    if len(origin) > 1:\n",
    "        raise ValueError(f\"Multiple origins found: {origin['id'].tolist()}\")\n",
    "    \n",
    "    return origin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>toid</th>\n",
       "      <th>vpuid</th>\n",
       "      <th>topo</th>\n",
       "      <th>hydroseq</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>wb-87646</td>\n",
       "      <td>nex-87404</td>\n",
       "      <td>02</td>\n",
       "      <td>fl-nex</td>\n",
       "      <td>26923.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         id       toid vpuid    topo  hydroseq\n",
       "1  wb-87646  nex-87404    02  fl-nex   26923.0"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "identifier = \"gages-01563500\"\n",
    "id_type = \"hl_uri\"\n",
    "origin = find_origin(network_table, identifier, id_type)\n",
    "origin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "network_df = tables[\"network\"].scan().to_pandas()\n",
    "nexus_df = tables[\"nexus\"].scan().to_pandas()\n",
    "\n",
    "terminal_id = origin['id'].iloc[0]\n",
    "terminal_nexus = origin['toid'].iloc[0]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
