{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0",
   "metadata": {},
   "source": [
    "# Hydrofabric Geopackage Viewer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1",
   "metadata": {},
   "source": [
    "Reads/extracts files from the NGWPC pyiceberg resources"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "import pandas as pd\n",
    "from ipywidgets import interact\n",
    "from pyiceberg.catalog import load_catalog\n",
    "\n",
    "from icefabric.helpers import load_creds, load_pyiceberg_config\n",
    "from icefabric.hydrofabric import subset_hydrofabric\n",
    "from icefabric.schemas import IdType\n",
    "from icefabric.ui import create_time_series_widget, get_hydrofabric_gages, get_streamflow_data\n",
    "\n",
    "# Changes the current working dir to be the project root\n",
    "current_working_dir = Path.cwd()\n",
    "os.chdir(Path.cwd() / \"../../\")\n",
    "print(\n",
    "    f\"Changed current working dir from {current_working_dir} to: {Path.cwd()}. This must run at the project root\"\n",
    ")\n",
    "\n",
    "\n",
    "# dir is where the .env file is located\n",
    "load_creds(dir=Path.cwd())\n",
    "\n",
    "# Loading the local pyiceberg config settings\n",
    "pyiceberg_config = load_pyiceberg_config(Path.cwd())\n",
    "catalog = load_catalog(\n",
    "    name=\"sql\",\n",
    "    type=pyiceberg_config[\"catalog\"][\"sql\"][\"type\"],\n",
    "    uri=pyiceberg_config[\"catalog\"][\"sql\"][\"uri\"],\n",
    "    warehouse=pyiceberg_config[\"catalog\"][\"sql\"][\"warehouse\"],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3",
   "metadata": {},
   "source": [
    "## Getting streamflow observations for different gages\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4",
   "metadata": {},
   "source": [
    "Step 1) getting the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pprint import pprint\n",
    "\n",
    "# Using a local warehouse for this example. This was created through the following command and NGWPC test AWS account\n",
    "# python tools/pyiceberg/export_catalog.py --namespace streamflow_observations\n",
    "streamflow_obs_df = get_streamflow_data(catalog_name=\"sql\", **pyiceberg_config[\"catalog\"])\n",
    "\n",
    "# List all gauge IDs\n",
    "\n",
    "pprint(streamflow_obs_df.columns.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# select the gauge ID you want to use:\n",
    "gage_id = \"12145500\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7",
   "metadata": {},
   "outputs": [],
   "source": [
    "streamflow_obs_df[\"time\"] = pd.to_datetime(streamflow_obs_df[\"time\"])\n",
    "\n",
    "# Scatter Plot of observations\n",
    "create_time_series_widget(streamflow_obs_df, point_size=5, time_col=\"time\", flow_col=gage_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8",
   "metadata": {},
   "source": [
    "## Geopackage Summary"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9",
   "metadata": {},
   "source": [
    "Print list of layers, number of catchments, and list of hydrolocations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10",
   "metadata": {},
   "outputs": [],
   "source": [
    "pprint(get_hydrofabric_gages(catalog=catalog))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using a local warehouse for this example. This was created through the following command and NGWPC test AWS account\n",
    "# python tools/pyiceberg/export_catalog.py --namespace conus_hf\n",
    "gage_id = \"11280000\"\n",
    "layers = [\"flowpaths\", \"nexus\", \"divides\", \"network\", \"hydrolocations\", \"pois\"]\n",
    "domain = \"conus_hf\"\n",
    "\n",
    "upstream_connections_path = Path.cwd() / f\"data/hydrofabric/{domain}_upstream_connections.json\"\n",
    "assert upstream_connections_path.exists(), (\n",
    "    f\"Upstream Connections missing for {domain}. Please run `icefabric build-upstream-connections` to generate this file\"\n",
    ")\n",
    "\n",
    "with open(upstream_connections_path) as f:\n",
    "    data = json.load(f)\n",
    "    print(\n",
    "        f\"Loading upstream connections connected generated on: {data['_metadata']['generated_at']} from snapshot id: {data['_metadata']['iceberg']['snapshot_id']}\"\n",
    "    )\n",
    "    upstream_dict = data[\"upstream_connections\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12",
   "metadata": {},
   "outputs": [],
   "source": [
    "layers_df = subset_hydrofabric(\n",
    "    catalog=catalog,\n",
    "    layers=layers,\n",
    "    identifier=f\"gages-{gage_id}\",\n",
    "    id_type=IdType.HL_URI,\n",
    "    namespace=\"conus_hf\",\n",
    "    upstream_dict=upstream_dict,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Layers:\")\n",
    "print(list(layers_df.keys()))\n",
    "row, col = layers_df[\"divides\"].shape\n",
    "print(\"Number of catchments:\")\n",
    "print(row)\n",
    "print(\"Hydrolocations:\")\n",
    "hl = layers_df[\"hydrolocations\"].hl_uri.tolist()\n",
    "hl_str = \", \".join(hl)\n",
    "print(hl_str)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14",
   "metadata": {},
   "source": [
    "## Map of divides, nexuses, and flowpaths"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15",
   "metadata": {},
   "source": [
    "Plot divides, nexuses, and flowpaths on a map."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16",
   "metadata": {},
   "outputs": [],
   "source": [
    "divides = layers_df[\"divides\"].to_crs(epsg=3857)\n",
    "\n",
    "flowpaths = layers_df[\"flowpaths\"].to_crs(epsg=3857)\n",
    "nexus = layers_df[\"nexus\"].to_crs(epsg=3857)\n",
    "\n",
    "div_ex = divides.explore()\n",
    "fl_ex = flowpaths.explore(m=div_ex, color=\"yellow\")\n",
    "nexus.explore(m=fl_ex, color=\"red\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17",
   "metadata": {},
   "source": [
    "## View Layers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18",
   "metadata": {},
   "source": [
    "Select layer and print table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Configure pandas display options for better formatting\n",
    "pd.set_option(\"display.max_columns\", None)\n",
    "pd.set_option(\"display.width\", None)\n",
    "pd.set_option(\"display.max_colwidth\", 50)\n",
    "pd.set_option(\"display.expand_frame_repr\", False)\n",
    "\n",
    "# Interactive display with limited rows\n",
    "interact(lambda layer_name: layers_df[layer_name].head(20), layer_name=layers)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "icefabric",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
